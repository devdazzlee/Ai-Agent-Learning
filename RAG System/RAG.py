# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13lVqSX8uXICyFTsUIaHQsuRX5h9zU-AM
"""

print("Function Calling")

for i in range(10):
  print(i)

i = 12
while  i < 20:
  print(i)
  i = i+1

# Text Image Audio Files

# Rag System
# R Reterival
# A Augment ==> Set Data in Prompt
# G Generation

# Tokenization model according to how much data is relevant for Rag system

# An embedding model is a type of machine‑learning model whose main job is to turn inputs (words, sentences, images, users, items, etc.) into fixed‑length vectors of numbers—called embeddings—that capture their essential properties and relationships.
#  You can think of an embedding as a point in a high‑dimensional space where “similar” things end up close together.

# Tokenization

# Splits raw text into tokens (words, subwords, or characters).

# Maps each token to a unique integer ID.

# Embedding model

# Takes those integer IDs (or the tokens) as input.

# Looks up or computes a fixed‑length real‑valued vector for each token (or aggregates them for a sentence/document).

# "unbelievable"
#   → ["un", "##believ", "##able"]
#   → [  834,      15237,    719  ]       (tokenizer)
#   → [0.12, –0.05, …, 0.02]              (embedding)

!pip install -q -U google-generativeai

import google.generativeai as genai
# from google.cloud import userdata

genai.configure(api_key="AIzaSyD4gRQkOAfWHix87V7Lo7fbms8OPdEJplE")

list(genai.list_models())

from typing import Dict

result : Dict = genai.embed_content(
    model="models/embedding-001",
    content="What is the meaning of Life?",
    task_type="retrieval_document",
    title="Embedding of Single String"
)

print("embedding")
print(result)
print(result["embedding"])

len(result["embedding"])

from typing import Dict

result : Dict = genai.embed_content(
    model="models/embedding-001",
    content=[
        "What is the meaning of Life?",
        "We Love Pakistan",
        "We Love USA",
        "Founder of PIAIC"
    ],
    task_type="retrieval_document",
    title="Embedding of Single String"
)

for embedding in result["embedding"]:
  print(str(embedding)[:50], "====Trimmed====" , len(embedding) )

"""Building Vector Stores & Retreival using Chroma DB"""

!pip install -Uq langchain-chroma

import getpass
import os

from langchain_core.documents import Document

documents = [
     Document(
        page_content="Lions are majestic predators known for their strength and complex social pride structures.",
        metadata={"source": "mammal-pet-doc", "id": "lion_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Elephants are the largest land animals, recognized for their remarkable intelligence and long memories.",
        metadata={"source": "mammal-pet-doc", "id": "elephant_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Cheetahs are the fastest land mammals, capable of reaching speeds up to 60–70 mph in short bursts.",
        metadata={"source": "mammal-pet-doc", "id": "cheetah_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Dolphins are highly social marine mammals that use sophisticated vocalizations to communicate and navigate.",
        metadata={"source": "mammal-pet-doc", "id": "dolphin_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Eagles are birds of prey with exceptional eyesight and powerful flight, often soaring at high altitudes.",
        metadata={"source": "mammal-pet-doc", "id": "eagle_doc"} # Adding an id to metadata
    ),
]

!pip uninstall -y langchain langchain-core langchain-google-genai

!pip uninstall -y langchain langchain-core langchain-google-genai
!pip install langchain langchain-google-genai

!pip uninstall -y langchain langchain-core langchain-google-genai
!pip install langchain==0.1.16 langchain-core==0.1.42 langchain-google-genai==0.0.9

from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key="AIzaSyD4gRQkOAfWHix87V7Lo7fbms8OPdEJplE"
)

print(embeddings.embed_query("Who is the Founder of Pakistan ??"))
len(embeddings.embed_query("Who is the Founder of Pakistan ??"))

from langchain_chroma import Chroma
from langchain_core.documents import Document

documents = [
     Document(
        page_content="Lions are majestic predators known for their strength and complex social pride structures.",
        metadata={"source": "mammal-pet-doc", "id": "lion_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Elephants are the largest land animals, recognized for their remarkable intelligence and long memories.",
        metadata={"source": "mammal-pet-doc", "id": "elephant_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Cheetahs are the fastest land mammals, capable of reaching speeds up to 60–70 mph in short bursts.",
        metadata={"source": "mammal-pet-doc", "id": "cheetah_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Dolphins are highly social marine mammals that use sophisticated vocalizations to communicate and navigate.",
        metadata={"source": "mammal-pet-doc", "id": "dolphin_doc"} # Adding an id to metadata
    ),
    Document(
        page_content="Eagles are birds of prey with exceptional eyesight and powerful flight, often soaring at high altitudes.",
        metadata={"source": "mammal-pet-doc", "id": "eagle_doc"} # Adding an id to metadata
    ),
]

# Extract ids from metadata
ids = [doc.metadata['id'] for doc in documents]

vectorestore = Chroma.from_documents(
    documents,
    embedding=embeddings,
    ids=ids # Pass ids explicitly
)

list(dir(vectorestore))

vectorestore.similarity_search_with_score("Tell me about Elephants?")

async def callFunction():
    await vectorestore.similarity_search("Tell me about Elephants?")

callFunction()

embedding = embeddings.embed_query("Tell me about Elephants?")
print(embedding)
vectorestore.similarity_search_by_vector(embedding)

len(embedding)

"""# **Reteriver **"""

from langchain_core.documents import Document
from langchain_core.runnables import RunnableLambda

retriever = RunnableLambda(vectorestore.similarity_search).bind(k=1)
retriever.batch(["Tell me about Lions?"])

"""### **Augment Mean**"""

from langchain_google_genai import ChatGoogleGenerativeAI
import os

# Set the API key as an environment variable
os.environ["GOOGLE_API_KEY"] = "AIzaSyD4gRQkOAfWHix87V7Lo7fbms8OPdEJplE"


llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
)
llm

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

message = """
Answer this question using the provided context only

{question}

Context:
{context}
"""

prompt = ChatPromptTemplate.from_messages([("human",message)])

"""**Rag**"""

rag_chain = {"context" : retriever, "question" : RunnablePassthrough()} | prompt | llm

response = rag_chain.invoke("Tell ma about Cheetahs?")
print(response.content)

!pip install -Uq facenet-pytorch

!pip install -Uq pillow

import torch
import torch.nn as nn
import torchvision.models as models

import torchvision.transforms as transforms
from PIL import Image

from facenet_pytorch import MTCNN, InceptionResnetV1
model = InceptionResnetV1(pretrained='vggface2').eval()
model

def preprocess_image(image_path):
  image = Image.open(image_path).convert("RGB")
  preprocess = transforms.Compose([
      transforms.Resize((224,224)),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
  ])
  return preprocess(image).unsqueeze(0)

def create_image_embedding(image_path):
  try:
    input_tensor = preprocess_image(image_path)
    with torch.no_grad():
      embeddings = model(input_tensor)
    return embeddings.squeeze().numpy()
  except Exception as e:
    print(f"Error creating embedding: {e}")  # Print the exception details
    return None

!mkdir images

import os
import requests

def save_image_from_url(image_url, image_name):
  """
  Dowload an image from a URL save it to 'images' folder

  Args:
    image_url: The URL of the image to download
    image_name: The Name of the file to save the image as
  """
  try:
    if not os.path.exists("images"):
      os.makedirs("images")

    image_path = os.path.join("images", image_name)

    response = requests.get(image_url, stream=True)
    response.raise_for_status()

    with open(image_path, "wb") as file:
      for chunk in response.iter_content(chunk_size=8192):
        file.write(chunk)

    print(f"Image saved at: {image_path}")
  except requests.exceptions.RequestException as e:
    print(f"Error downloading image: {e}")
  except Exception as e:
    print(f"Error Occur in saving image: {e}")

save_image_from_url("https://images.unsplash.com/photo-1579353977828-2a4eab540b9a?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8c2FtcGxlfGVufDB8fDB8fHww" , "jetha1.jpg")
save_image_from_url("https://images.unsplash.com/photo-1579353977828-2a4eab540b9a?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8c2FtcGxlfGVufDB8fDB8fHww" , "jetha2.jpg")
save_image_from_url("https://images.unsplash.com/photo-1579353977828-2a4eab540b9a?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8c2FtcGxlfGVufDB8fDB8fHww" , "jetha3.jpg")

image_path = "./images/jetha1.jpg"
print(image_path)
q2 = create_image_embedding(image_path)
print(q2.shape)
print(q2)

r1 = create_image_embedding('./images/jetha1.jpg')
r2 = create_image_embedding('./images/jetha2.jpg')
r3 = create_image_embedding('./images/jetha3.jpg')

!pip install -U milvus-lite

!pip install -U pymilvus

from pymilvus import MilvusClient
client = MilvusClient('./milvus_demo.db')
client.list_collections()

from pymilvus import MilvusClient
import numpy as np

client = MilvusClient('./milvus_demo.db')
client.create_collection(
    collection_name="images",
    dimension=512,
)


r1 = create_image_embedding('./images/jetha1.jpg')
r2 = create_image_embedding('./images/jetha2.jpg')
r3 = create_image_embedding('./images/jetha3.jpg')

data = [
  {"id"  : 1, "person_name" : f"jetha 1" , "vector" : r1}, # Change id to "id"
  {"id"  : 2, "person_name" : f"jetha 2" , "vector" : r2}, # Change id to "id"
  {"id"  : 3, "person_name" : f"jetha 3" , "vector" : r3}, # Change id to "id"
]

res  = client.insert(
    collection_name="images",
    data=data
)

res = client.search(
    collection_name="images",
    data=[r1],
    limit=1,
    output_fields=["id" , "person_name"]
)
print(res)

v4 = create_image_embedding("./images/jetha1.jpg")
v4

res = client.search(
    collection_name="images",
    data=[v4],
    limit=1,
    output_fields=["id" , "person_name"]
)
print(res)